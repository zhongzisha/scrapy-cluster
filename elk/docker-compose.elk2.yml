version: '2'
# This compose file stands up Scrapy Cluster with an
# associated ELK Stack. You should run a few crawls and then import the
# `export.json` file into your Kibana objects

services:
  kafka_monitor:
    image: istresearch/scrapy-cluster:kafka-monitor-dev
    volumes:
      - logs:/usr/src/app/logs
    environment:
      - LOG_STDOUT=False
      - LOG_USE_JSON=True
    depends_on:
      - kafka-service
      - redis-service
    restart: always
  redis_monitor:
    image: istresearch/scrapy-cluster:redis-monitor-dev
    volumes:
      - logs:/usr/src/app/logs
    environment:
      - LOG_STDOUT=False
      - LOG_USE_JSON=True
    depends_on:
      - kafka-service
      - redis-service
      - zookeeper-service
    restart: always
  crawler:
    image: istresearch/scrapy-cluster:crawler-dev
    volumes:
      - logs:/usr/src/app/logs
    environment:
      - SC_LOG_STDOUT=False
      - SC_LOG_JSON=True
    depends_on:
      - kafka-service
      - redis-service
      - zookeeper-service
    restart: always
  rest:
    image: istresearch/scrapy-cluster:rest-dev
    volumes:
      - logs:/usr/src/app/logs
    depends_on:
      - kafka-service
      - redis-service
    restart: always
    ports:
      - "5343:5343"
    environment:
      - LOG_STDOUT=False
      - LOG_USE_JSON=True
  redis-service:
    image: redis
    ports:
      - "6379"
    restart: always
  zookeeper-service:
    image: zookeeper
    ports:
      - "2181:2181"
    restart: always
  kafka-service:
    image: wurstmeister/kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-service:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:9092
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_BROKER_ID: 1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper-service
    restart: always
  elasticsearch:
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
    environment:
      - network.host=0.0.0.0
      - discovery.type=single-node
    ports:
      - "9200:9200"
      - "9300:9300"
  logstash:
    image: "docker.elastic.co/logstash/logstash:7.9.3"
    command: logstash -f /etc/logstash/conf.d/logstash.conf
    environment:
      - "LS_JVM_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.security=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.security.cert=ALL-UNNAMED --add-opens=java.base/java.util.zip=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.util.regex=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/javax.crypto=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED"
    volumes:
      - ./scrapy-cluster-logstash-docker.conf:/etc/logstash/conf.d/logstash.conf
      - ./logs-template.json:/etc/logstash/templates/logs-template.json
      - logs:/var/log/scrapy-cluster
    ports:
      - "5000:5000"
    links:
      - elasticsearch
  kibana:
    image: "docker.elastic.co/kibana/kibana:7.9.3"
    ports:
      - "5601:5601"
    links:
      - elasticsearch

volumes:
  logs:
